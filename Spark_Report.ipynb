{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *RDD: Resilient Distributed Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Resilient__: withstand failures and complete an ongoing computation (fault tolerance).\n",
    "- __Distributed__: spanning across multiple machines.\n",
    "- __Datasets__: hold data.\n",
    "\n",
    "RDD is a __read-only (immutable)__, partitioned collection of records\n",
    "RDDs are made up of 4 parts:\n",
    "\n",
    "- __Partitions__: Atomic pieces of the dataset. One or many per compute node.\n",
    "- __Dependencies__: Models relationship between this RDD and its partitions with the RDD(s) it was derived from.\n",
    "- A __function__ for computing the dataset based on its parent RDDs.\n",
    "- __Metadata__ about it partitioning scheme and data placement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Difference between Hadoop and Spark\n",
    "Hadoop and Apache Spark are both big-data frameworks, but they don't really serve the same purposes.\n",
    "### Hadoop:\n",
    "- Hadoop is essentially a distributed data infrastructure: It distributes massive data collections across multiple nodes within a cluster of commodity servers, which means you don't need to buy and maintain expensive custom hardware.\n",
    "- Hadoop has to read from and write to a disk.\n",
    "- Hadoop written in __Java__.\n",
    "\n",
    "### Spark:\n",
    "- A data-processing tool that operates on those distributed data collections, it doesn't do distributed storage.\n",
    "- Spark process data __in-memory database__.\n",
    "- Spark written in __Scala__.\n",
    "#### => Spark up to ~100 times faster\n",
    "#### => Hadoop is able to work with larger datasets than Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformation allow to create new RDDs from the existing RDDs (__not modified__ the existing RDD because __immutable__)\n",
    "- New RDD may larger than old RDD (union, cartesian)\n",
    "- New RDD may smaller than old RDD (filter, count, distinc, sample)\n",
    "- New RDD may have a same size with old RDD (map, flatMap)\n",
    "- Transformations are __lazy__, they get execute when we call an action. They are not executed immediately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://raw.githubusercontent.com/hongducphan/spark_intro/master/narrow_vs_wide_dependencies.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Map\n",
    "Return a new RDD by applying a function to each element of this RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a', 'c']\n",
      "[('b', 1), ('a', 1), ('c', 1)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize(['b', 'a', 'c'])\n",
    "y = x.map(lambda z: (z, 1))\n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Filter\n",
    "Return a new RDD containing only the elements that satisfy a predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1, 2, 3])\n",
    "y = x.filter(lambda x: x % 2 == 1) # keep old values\n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Flatmap\n",
    "Return a new RDD by first applying a function to all elements of this RDD, an then flattening the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 100, 42, 2, 200, 42, 3, 300, 42]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1, 2, 3])\n",
    "y = x.flatMap(lambda x: (x, x*100, 42))\n",
    "             \n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Group by\n",
    "Group data in the original RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('D', ['Duc']), ('K', ['Khanh']), ('M', ['Minh'])]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize(['Duc', 'Khanh', 'Minh'])\n",
    "y = x.groupBy(lambda x: x[0])\n",
    "\n",
    "print([(k, list(v)) for (k, v) in y.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Keyed transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Group By Key\n",
    "![Image of Yaktocat](https://raw.githubusercontent.com/hongducphan/spark_intro/master/grbykey.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1)]\n",
      "[('A', [3, 2, 1]), ('B', [5, 4])]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1)])\n",
    "y = x.groupByKey()\n",
    "             \n",
    "print(x.collect())\n",
    "print(list((j[0], list(j[1])) for j in y.collect()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Reduce By Key\n",
    "![Image of Yaktocat](https://raw.githubusercontent.com/hongducphan/spark_intro/master/reducebykeky.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1)]\n",
      "[('A', 6), ('B', 9)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1)])\n",
    "y = x.reduceByKey(lambda x, y: x + y)\n",
    "             \n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Join\n",
    "![Image of Yaktocat](https://raw.githubusercontent.com/hongducphan/spark_intro/master/join.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', (2, 5)), ('a', (1, 3)), ('a', (1, 4))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([('a', 1), ('b', 2)])\n",
    "y = sc.parallelize([('a', 3), ('a', 4), ('b', 5)])\n",
    "z = x.join(y)\n",
    "\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,3,4])\n",
    "y = x.distinct()\n",
    "\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Action\n",
    "- Actions use to trigger computation and processing of the dataset\n",
    "- Actions are executed on executors and they pass results back to the driver\n",
    "- Actions are used to collect, save, print and fold data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://raw.githubusercontent.com/hongducphan/spark_intro/master/action1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3], 2)\n",
    "y = x.collect()\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4])\n",
    "y = x.reduce(lambda a, b: a + b)\n",
    "\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 3]\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([2, 4, 3])\n",
    "y = x.mean()\n",
    "\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1]\n",
      "1.247219128924647\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([2, 4, 1])\n",
    "y = x.stdev()\n",
    "\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://raw.githubusercontent.com/hongducphan/spark_intro/master/action2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Spark execution and scheduling\n",
    "- __Spark context__: for sets up internal services and establish connection to Spark execution environment.\n",
    "- Can tracking __current status__ of Spark application like __configuration, app name__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast variable:\n",
    "- Read-only shared variables with effective sharing mechanism.\n",
    "\n",
    "### Accumulator variable:\n",
    "- Read-write shared variables with restricted updates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
